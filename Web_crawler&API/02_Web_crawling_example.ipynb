{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbY8GI3njoXOjbaLojjEKk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Liping-LZ/BDAO_DSDO/blob/main/Web_crawler%26API/02_Web_crawling_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before we start\n",
        "\n",
        "This notebook is showing you how you can use Python to crawl a website and get all the links.\n",
        "\n",
        "Web crawling is the process of systematically browsing and discovering web pages on the internet using automated software called web crawlers or spiders.\n",
        "\n",
        "It aims to systematically browse and follow links to find as many web page as possible.\n",
        "\n",
        "Here we will use requests and BeautifulSoup to crawl and extract links from a website https://books.toscrape.com/. This website is built for scraping for learning purpose.\n",
        "\n",
        "This is just a simple example. In real practice, crawling can be very complicated since the real business website can have more complex structure.\n",
        "\n",
        "In addition, web crawling can be used for efficient web optimisation. We are continue this topic on Week 4."
      ],
      "metadata": {
        "id": "IxYdbDTgDZjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Import the relevant libraries"
      ],
      "metadata": {
        "id": "kmwLKVepFJf0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RygUP6wnDV_1"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Define the base URL, and a set for storing visited URLs and a list to store URLs to visit"
      ],
      "metadata": {
        "id": "YJKKcjfLFYIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL of the website to crawl\n",
        "base_url = \"https://books.toscrape.com/\"\n",
        "\n",
        "# Set to store visited URLs\n",
        "visited_urls = set()\n",
        "\n",
        "# List to store URLs to visit next\n",
        "urls_to_visit = [base_url]"
      ],
      "metadata": {
        "id": "dVWdvIKyFm4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Write a function to crawl each web page and extract the links."
      ],
      "metadata": {
        "id": "3G9795kZFwbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`urllib.parse.urljoin`\n",
        "\n",
        "Construct a full (“absolute”) URL by combining a “base URL” (base) with another URL (url). Informally, this uses components of the base URL, in particular the addressing scheme, the network location and (part of) the path, to provide missing components in the relative URL.\n",
        "\n",
        "For example:\n",
        "\n",
        ">from urllib.parse import urljoin\n",
        "\n",
        ">urljoin('http://www.cwi.nl/%7Eguido/Python.html', 'FAQ.html')\n",
        "\n",
        "\n",
        "Output: 'http://www.cwi.nl/%7Eguido/FAQ.html' [link text](https://)"
      ],
      "metadata": {
        "id": "on8zqnFnHG9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to crawl a page and extract links\n",
        "def crawl_page(url):\n",
        "    #here we use try & except to better handle request errors\n",
        "    try:\n",
        "        response = requests.get(url) # Send HTTP request to the server of the target url\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\") # use beautifulsoup to parse the html page\n",
        "\n",
        "        # Extract links and enqueue new URLs\n",
        "        links = [] # create an empty list for links\n",
        "        for link in soup.find_all(\"a\", href=True): # since in html links are usually in <a> tags 'href' attribute, here we use .find_all(\"a\", href=True) to get all links\n",
        "            next_url = urljoin(url, link[\"href\"])  # get the new link from the page\n",
        "            links.append(next_url) # add the new link to the links list\n",
        "\n",
        "        return links\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error crawling {url}: {e}\") # if there is request error, then print \"error crawling\"\n",
        "        return []"
      ],
      "metadata": {
        "id": "6XFDyrx_F_y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 Start to crawl the website"
      ],
      "metadata": {
        "id": "OYJxxOSeICKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crawl the website\n",
        "while urls_to_visit: # when urls_to_visit list is not empty, continue\n",
        "    current_url = urls_to_visit.pop(0)  # Dequeue the first URL\n",
        "\n",
        "    if current_url in visited_urls:\n",
        "        continue\n",
        "\n",
        "    print(f\"Crawling: {current_url}\")\n",
        "\n",
        "    new_links = crawl_page(current_url) # get all the new links by crawl the current page\n",
        "    visited_urls.add(current_url) # add the current url to visited url set\n",
        "    urls_to_visit.extend(new_links) # add the new links to urls_to_visit list\n",
        "\n",
        "print(\"Crawling finished.\") # when there are no more new links, crawling finish"
      ],
      "metadata": {
        "id": "3sMn0bc4IIfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5 Store the links into dataframe"
      ],
      "metadata": {
        "id": "6l3eqeZmI57n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "links = pd.DataFrame(visited_urls, columns = ['links'])\n",
        "links"
      ],
      "metadata": {
        "id": "glf6daAPI-UA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The end.**"
      ],
      "metadata": {
        "id": "g8ZuudOVJAOS"
      }
    }
  ]
}